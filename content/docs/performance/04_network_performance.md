---
title: Linux 网络
weight: 4
---

# Linux 网络

## 网络模型

OSI 网络模型是国际标准化组织制定的开放式系统互联通信参考模型（Open System Interconnection Reference Model），分为七层。应用层、表示层、会话层、传输层、网络层、数据链路层以及物理层。但是 OSI 模型还是太复杂了，实际上使用的是另一个更实用的四层模型，即 TCP/IP 网络模型。

TCP/IP 模型，把网络互联的框架分为应用层、传输层、网络层、网络接口层等四层。

- 应用层，负责向用户提供一组应用程序，比如 HTTP、FTP、DNS 等。
- 传输层，负责端到端的通信，比如 TCP、UDP 等。
- 网络层，负责网络包的封装、寻址和路由，比如 IP、ICMP 等。
- 网络接口层，负责网络包在物理网络中的传输，比如 MAC 寻址、错误侦测以及通过网卡传输网络帧等。

Linux 内核中的网络栈，其实也类似于 TCP/IP 的四层结构。

- 最上层的应用程序，需要通过系统调用，来跟套接字接口进行交互；
- 套接字的下面，就是我们前面提到的传输层、网络层和网络接口层；
- 最底层，则是网卡驱动程序以及物理网卡设备。

### Linux 网络包的接收流程

当一个网络帧到达网卡后，网卡会通过 DMA 方式，把这个网络包放到收包队列中；然后通过**硬中断**，告诉中断处理程序已经收到了网络包。

接着，网卡中断处理程序会为网络帧分配内核数据结构（sk_buff），并将其拷贝到 sk_buff 缓冲区中；然后再通过**软中断**，通知内核收到了新的网络帧。

接下来，内核协议栈从缓冲区中取出网络帧，并通过网络协议栈，从下到上逐层处理这个网络帧。比如，

- 在链路层检查报文的合法性，找出上层协议的类型（比如 IPv4 还是 IPv6），再去掉帧头、帧尾，然后交给网络层。
- 网络层取出 IP 头，判断网络包下一步的走向，比如是交给上层处理还是转发。当网络层确认这个包是要发送到本机后，就会取出上层协议的类型（比如 TCP 还是 UDP），去掉 IP 头，再交给传输层处理。
- 传输层取出 TCP 头或者 UDP 头后，根据 < 源 IP、源端口、目的 IP、目的端口 > 四元组作为标识，找出对应的 Socket，并把数据拷贝到 Socket 的接收缓存中。
- 最后，应用程序就可以使用 Socket 接口，读取到新接收到的数据了。

### Linux 网络包的发送流程

- 首先，应用程序调用 Socket API（比如 sendmsg）发送网络包。
- 由于这是一个系统调用，所以会陷入到内核态的套接字层中。套接字层会把数据包放到 Socket 发送缓冲区中。
- 接下来，网络协议栈从 Socket 发送缓冲区中，取出数据包；再按照 TCP/IP 栈，从上到下逐层处理。比如，传输层和网络层，分别为其增加 TCP 头和 IP 头，执行路由查找确认下一跳的 IP，并按照 MTU 大小进行分片。
- 分片后的网络包，再送到网络接口层，进行物理地址寻址，以找到下一跳的 MAC 地址。然后添加帧头和帧尾，放到发包队列中。这一切完成后，会有**软中断**通知驱动程序：发包队列中有新的网络帧需要发送。
- 最后，驱动程序通过 DMA ，从发包队列中读出网络帧，并通过物理网卡把它发送出去。

### 网络收发过程中缓冲区的位置

- 网卡收发网络包时，通过 DMA 方式交互的**环形缓冲区**；
- 网卡中断处理程序为网络帧分配的，内核数据结构 `sk_buff` 缓冲区；
- 应用程序通过套接字接口，与网络协议栈交互时的**套接字缓冲区**。

不过相应的，就会有两个问题。

首先，这些缓冲区的位置在哪儿？是在网卡硬件中，还是在内存中？

这些缓冲区都处于内核管理的内存中。

其中，环形缓冲区，由于需要 DMA 与网卡交互，理应属于网卡设备驱动的范围。

`sk_buff` 缓冲区，是一个维护网络帧结构的双向链表，链表中的每一个元素都是一个网络帧（Packet）。虽然 TCP/IP 协议栈分了好几层，但上下不同层之间的传递，实际上只需要操作这个数据结构中的指针，而无需进行数据复制。

套接字缓冲区，则允许应用程序，给每个套接字配置不同大小的接收或发送缓冲区。应用程序发送数据，实际上就是将数据写入缓冲区；而接收数据，其实就是从缓冲区中读取。至于缓冲区中数据的进一步处理，则由传输层的 TCP 或 UDP 协议来完成。

其次，这些缓冲区，跟 Buffer 和 Cache 有什么关联？

Buffer ，都跟块设备直接相关；而其他的都是 Cache。

实际上，`sk_buff`、套接字缓冲、连接跟踪等，都通过 slab 分配器来管理。你可以直接通过 `/proc/slabinfo`，来查看它们占用的内存大小。

### 内核协议栈，是通过一个内核线程的方式来运行的吗

软中断处理，就有专门的内核线程 ksoftirqd。每个 CPU 都会绑定一个 ksoftirqd 内核线程，比如， 2 个 CPU 时，就会有 ksoftirqd/0 和 ksoftirqd/1 这两个内核线程。

并非所有网络功能，都在软中断内核线程中处理。内核中还有很多其他机制（比如硬中断、kworker、slab 等），这些机制一起协同工作，才能保证整个网络协议栈的正常运行。

### 最大连接数

无论 TCP 还是 UDP，端口号都只占 16 位，也就说其最大值也只有 65535。

Linux 协议栈，通过五元组来标志一个连接（即协议，源 IP、源端口、目的 IP、目的端口)。

对客户端来说，每次发起 TCP 连接请求时，都需要分配一个空闲的本地端口，去连接远端的服务器。由于这个本地端口是独占的，所以客户端最多只能发起 65535 个连接。

对服务器端来说，其通常监听在固定端口上（比如 80 端口），等待客户端的连接。根据五元组结构，我们知道，客户端的 IP 和端口都是可变的。如果不考虑 IP 地址分类以及资源限制，服务器端的理论最大连接数，可以达到 2 的 48 次方（IP 为 32 位，端口号为 16 位），远大于 65535。

## 性能指标

- 带宽，表示链路的最大传输速率，单位通常为 `b/s` （`比特/秒`）。常用的带宽有 1000M、10G、40G、100G 等。
- 吞吐量，表示单位时间内成功传输的数据量，单位通常为 `b/s`（`比特/秒`）或者 `B/s`（`字节/秒`）。吞吐量受带宽限制，而`吞吐量 / 带宽`，也就是该网络的使用率。
- 延时，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。在不同场景中，这一指标可能会有不同含义。比如，它可以表示，建立连接需要的时间（比如 TCP 握手延时），或一个数据包往返所需的时间（比如 RTT）。
- PPS，是 Packet Per Second（`包/秒`）的缩写，表示以网络包为单位的传输速率。PPS 通常用来评估网络的转发能力，比如硬件交换机，通常可以达到线性转发（即 PPS 可以达到或者接近理论最大值）。而基于 Linux 服务器的转发，则容易受网络包大小的影响。

最后的 PPS，则是以网络包为单位的网络传输速率，通常用在需要大量转发的场景中。而对 TCP 或者 Web 服务来说，更多会用并发连接数和每秒请求数（QPS，Query per Second）等指标，它们更能反应实际应用程序的性能。

网络的可用性（网络能否正常通信）、并发连接数（TCP 连接数量）、丢包率（丢包百分比）、重传率（重新传输的网络包比例）等也是常用的性能指标。

## C10K 和 C1000K

C10K 和 C1000K 的字母 C 是 Client 的意思。C10K 就是单机同时处理 1 万个请求（并发连接 1 万）的问题，而 C1000K 就是单机处理 100 万个请求（并发连接 100 万）的问

### C10K

在 C10K 以前，Linux 中网络处理都用同步阻塞的方式，也就是每个请求都分配一个进程或者线程。请求数只有 100 个时，这种方式自然没问题，但增加到 10000 个请求时，10000 个进程或线程的调度、上下文切换乃至它们占用的内存，都会成为瓶颈。

解决思路就是异步、非阻塞 IO。

#### IO 模型优化

IO 多路复用（IO Multiplexing）。IO 多路复用是什么意思呢？

IO 事件通知的方式：

- 水平触发（LT）：应用程序可以随时检查文件描述符的状态，然后再根据状态，进行 IO 操作。
- 边缘触发（ET）：只有在文件描述符的状态发生改变（也就是 IO 请求达到）时，才发送一次通知。

IO 多路复用的实现方法：

- 使用非阻塞 IO 和水平触发通知，如 select 和 poll。select 和 poll 需要从文件描述符列表中，找出哪些可以执行 IO ，然后进行真正的网络 IO 读写。由于 IO 是非阻塞的，一个线程中就可以同时监控一批套接字的文件描述符，这样就达到了单线程处理多请求的目的。但是，应用软件使用 select 和 poll 时，需要对这些文件描述符列表进行轮询，这样，请求数多的时候就会比较耗时。select 使用固定长度的位相量，表示文件描述符的集合，因此会有最大描述符数量的限制。比如，在 32 位系统中，默认限制是 1024。而 poll 改进了 select 的表示方法，换成了一个没有固定长度的数组，这样就没有了最大描述符数量的限制（当然还会受到系统文件描述符限制）。但应用程序在使用 poll 时，同样需要对文件描述符列表进行轮询。应用程序每次调用 select 和 poll 时，还需要把文件描述符的集合，从用户空间传入内核空间，由内核修改后，再传出到用户空间中。这一来一回的内核空间与用户空间切换，也增加了处理成本。
- 使用非阻塞 I/O 和边缘触发通知，比如 epoll。epoll 解决了 select 和 poll 的问题。epoll 使用红黑树，在内核中管理文件描述符的集合，这样，就不需要应用程序在每次操作时都传入、传出这个集合。epoll 使用事件驱动的机制，只关注有 IO 事件发生的文件描述符，不需要轮询扫描整个集合。
- 使用异步 IO（Asynchronous IO，简称为 AIO）

#### 工作模型优化

使用 IO 多路复用后，就可以在一个进程或线程中处理多个请求，其中，又有下面两种不同的工作模型。

- 主进程 + 多个 worker 子进程，这也是最常用的一种模型。
  - 主进程执行 `bind() + listen()` 后，创建多个子进程；
  - 然后，在每个子进程中，都通过 `accept()` 或 `epoll_wait()` ，来处理相同的套接字。
- 监听到相同端口的多进程模型

最常见的反向代理服务器 Nginx 就是使用第一种模型。主进程主要用来初始化套接字，并管理子进程的生命周期；而 worker 进程，则负责实际的请求处理。但是 `accept()` 和 `epoll_wait()` 调用存在惊群问题。当网络 IO 事件发生时，多个进程会被唤醒，但是实际上只有一个进程来响应这个事件，其他进程重新休眠。为了避免惊群，Nginx 在每个进程中增加了一个全局锁。子进程需要竞争到锁以后才会加入到 epoll 中，这样就只有一个 worker 被唤醒。

epoll 会在Linux内核中申请一个简易的文件系统：

1. 调用epoll_create创建一个epoll对象（在epoll文件系统中给这个句柄分配资源，一棵红黑树和一个准备就绪list链表）。
2. 调用epoll_ctl向epoll对象中添加这100万个连接的套接字。就是把socket放到红黑树上，给内核中断处理程序注册一个回调函数，然后告诉内核如果这个句柄的中断到了，就把这个socket放到准备就绪list链表里。
3.调用epoll_wait收集发生事件的TCP连接。到准备就绪list链表中处理socket，并把数据返回给用户。

这样，只需要在进程启动的时候建立1个epoll对象，并在需要的时候向它添加或删除连接即可。因此在实际收集事件时，epoll_wait的效率会非常高，因为调用epoll_wait时并没有向它传递这100万个连接，内核也不需要去遍历所有的连接，只需要到就绪list链表中去处理socket就行了。

<https://www.cnblogs.com/skyfsm/p/7102367.html>

epoll在实现上的三个核心点是：1、mmap，2、红黑树，3、rdlist(就绪描述符链表)

mmap
    mmap是共享内存，用户进程和内核有一段地址(虚拟存储器地址)映射到了同一块物理地址上，这样当内核要对描述符上的事件进行检查的时候就不用来回的拷贝了。

红黑树
    红黑树是用来存储这些描述符的。当内核初始化epoll的时候（当调用epoll_create的时候内核也是个epoll描述符创建了一个文件，毕竟在Linux中一切都是文件，而epoll面对的是一个特殊的文件，和普通文件不同），会开辟出一块内核缓冲区，这块区域用来存储我们要监管的所有的socket描述符，当然在这里面存储有一个数据结构，这就是红黑树，由于红黑树的接近平衡的查找，插入，删除能力，在这里显著的提高了对描述符的管理。

rdlist
        rdlist就绪描述符链表这是一个双链表，epoll_wait()函数返回的也是这个就绪链表。当内核创建了红黑树之后，同时也会建立一个双向链表rdlist，用于存储准备就绪的描述符，当调用epoll_wait的时候在timeout时间内，只是简单的去管理这个rdlist中是否有数据，如果没有则睡眠至超时，如果有数据则立即返回并将链表中的数据赋值到events数组中。这样就能够高效的管理就绪的描述符，而不用去轮询所有的描述符。

        当执行epoll_ctl时除了把socket描述符放入到红黑树中之外，还会给内核中断处理程序注册一个回调函数，告诉内核，当这个描述符上有事件到达（或者说中断了）的时候就调用这个回调函数。这个回调函数的作用就是将描述符放入到rdlist中，所以当一个socket上的数据到达的时候内核就会把网卡上的数据复制到内核，然后把socket描述符插入就绪链表rdlist中。

node.js 底层
IO 分为网络 IO 和磁盘 IO，对于网络 IO，使用 epoll 之类的就可以了。但是对于磁盘 IO，没有完美的办法，所以都是使用多线程阻塞模拟的，不同在于 Windows 下的 IOCP 是在系统内核里提供的线程池，而 Linux 之类的在用户层提供的线程池。libeio 和 libev 是 node 较早版本使用的，在 libuv 提供之后，这两个库均不在使用。

### C1000K

100 万个请求需要大量的系统资源。比如，

假设每个请求需要 16KB 内存的话，那么总共就需要大约 15 GB 内存。

而从带宽上来说，假设只有 20% 活跃连接，即使每个连接只需要 1KB/s 的吞吐量，总共也需要 1.6 Gb/s 的吞吐量。千兆网卡显然满足不了这么大的吞吐量，所以还需要配置万兆网卡，或者基于多网卡 Bonding 承载更大的吞吐量。

其次，从软件资源上来说，大量的连接也会占用大量的软件资源，比如文件描述符的数量、连接状态的跟踪（CONNTRACK）、网络协议栈的缓存大小（比如套接字读写缓存、TCP 读写缓存）等等。

大量请求带来的中断处理，也会带来非常高的处理成本。

C1000K 的解决方法，本质上还是构建在 epoll 的非阻塞 IO 模型上。只不过，除了 IO 模型之外，还需要从应用程序到 Linux 内核、再到 CPU、内存和网络等各个层次的深度优化，特别是需要借助硬件，来卸载那些原来通过软件处理的大量功能。

### C10M

要实现 C10M ，就不只是增加物理资源，或者优化内核和应用程序可以解决的问题了。这时候，就需要用 XDP 的方式，在内核协议栈之前处理网络包；或者用 DPDK 直接跳过网络协议栈，在用户空间通过轮询的方式直接处理网络包。

DPDK 是目前最主流的高性能网络方案，不过，这需要能支持 DPDK 的网卡配合使用。

在大多数场景中，我们并不需要单机并发 1000 万请求。通过调整系统架构，把请求分发到多台服务器中并行处理，才是更简单、扩展性更好的方案。

### DNS 解析分析

DNS 服务器的配置：

```bash
[root@SGDLITVM0905 ~]# cat /etc/resolv.conf
nameserver 16.187.185.201
```

nslookup 命令，可以查询到这个域名的 A 记录：

```bash
[root@SGDLITVM0905 ~]# nslookup www.baidu.com
Server:  16.187.185.201
Address: 16.187.185.201#53

Non-authoritative answer:
www.baidu.com canonical name = www.a.shifen.com.
www.a.shifen.com canonical name = www.wshifen.com.
Name: www.wshifen.com
Address: 104.193.88.77
Name: www.wshifen.com
Address: 104.193.88.123
```

另外一个常用的 DNS 解析工具 dig ，提供了 trace 功能，可以展示递归查询的整个过程：

```bash
# +trace 表示开启跟踪查询
# +nodnssec 表示禁止 DNS 安全扩展
$ dig +trace +nodnssec time.geekbang.org

; <<>> DiG 9.11.3-1ubuntu1.3-Ubuntu <<>> +trace +nodnssec time.geekbang.org
;; global options: +cmd
.   322086 IN NS m.root-servers.net.
.   322086 IN NS a.root-servers.net.
.   322086 IN NS i.root-servers.net.
.   322086 IN NS d.root-servers.net.
.   322086 IN NS g.root-servers.net.
.   322086 IN NS l.root-servers.net.
.   322086 IN NS c.root-servers.net.
.   322086 IN NS b.root-servers.net.
.   322086 IN NS h.root-servers.net.
.   322086 IN NS e.root-servers.net.
.   322086 IN NS k.root-servers.net.
.   322086 IN NS j.root-servers.net.
.   322086 IN NS f.root-servers.net.
;; Received 239 bytes from 114.114.114.114#53(114.114.114.114) in 1340 ms

org.   172800 IN NS a0.org.afilias-nst.info.
org.   172800 IN NS a2.org.afilias-nst.info.
org.   172800 IN NS b0.org.afilias-nst.org.
org.   172800 IN NS b2.org.afilias-nst.org.
org.   172800 IN NS c0.org.afilias-nst.info.
org.   172800 IN NS d0.org.afilias-nst.org.
;; Received 448 bytes from 198.97.190.53#53(h.root-servers.net) in 708 ms

geekbang.org.  86400 IN NS dns9.hichina.com.
geekbang.org.  86400 IN NS dns10.hichina.com.
;; Received 96 bytes from 199.19.54.1#53(b0.org.afilias-nst.org) in 1833 ms

time.geekbang.org. 600 IN A 39.106.233.176
;; Received 62 bytes from 140.205.41.16#53(dns10.hichina.com) in 4 ms
```

第一部分，是从 `114.114.114.114` 查到的一些根域名服务器（`.`）的 NS 记录。

第二部分，是从 NS 记录结果中选一个（`h.root-servers.net`），并查询顶级域名 `org.` 的 NS 记录。

第三部分，是从 `org.` 的 NS 记录中选择一个（`b0.org.afilias-nst.org`），并查询二级域名 `geekbang.org.` 的 NS 服务器。

最后一部分，就是从 `geekbang.org.` 的 NS 服务器（`dns10.hichina.com`）查询最终主机 `time.geekbang.org.` 的 A 记录。

#### DNS 解析慢

dnsmasq 是最常用的 DNS 缓存服务之一，还经常作为 DHCP 服务来使用。性能也可以满足绝大多数应用程序对 DNS 缓存的需求。

### tcpdump 和 wireshark

tcpdump 和 Wireshark 就是最常用的网络抓包和分析工具

- tcpdump 仅支持命令行格式使用，常用在服务器中抓取和分析网络包。
- Wireshark 除了可以抓包外，还提供了强大的图形界面和汇总分析工具，简单实用。

### DDoS

DDoS 的前身是 DoS（Denail of Service），即拒绝服务攻击，指利用大量的合理请求，来占用过多的目标资源，从而使目标服务无法响应正常请求。

DDoS（Distributed Denial of Service） 则是在 DoS 的基础上，采用了分布式架构，利用多台主机同时攻击目标主机。这样，即使目标服务部署了网络防御设备，面对大量网络请求时，还是无力应对。

DDoS 可以分为下面几种类型。

第一种，耗尽带宽。无论是服务器还是路由器、交换机等网络设备，带宽都有固定的上限。带宽耗尽后，就会发生网络拥堵，从而无法传输其他正常的网络报文。

第二种，耗尽操作系统的资源。网络服务的正常运行，都需要一定的系统资源，像是 CPU、内存等物理资源，以及连接表等软件资源。一旦资源耗尽，系统就不能处理其他正常的网络连接。

第三种，消耗应用程序的运行资源。应用程序的运行，通常还需要跟其他的资源或系统交互。如果应用程序一直忙于处理无效请求，也会导致正常请求的处理变慢，甚至得不到响应。
