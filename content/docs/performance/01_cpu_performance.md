---
title: CPU 性能优化
weight: 1
---

从**应用负载的视角**来考察性能：**吞吐**和**延时**，对应着高并发和响应快

从**系统资源的视角**考察的指标：资源使用率，饱和度等。

性能分析就是**找出应用或者系统的瓶颈，并设法去避免或缓解它们**。可以分为六个步骤：

1. **选择指标**评估应用程序和系统的性能
2. 为应用程序和系统**设置性能目标**
3. 进行性能**基准测试**
4. 性能分析**定位瓶颈**
5. **优化**系统和应用程序
6. 性能**监控和告警**

## 平均负载

查看系统负载，使用 `top` 或 `uptime` 命令：

```bash
[root@shccdfrh75vm8 dev]# uptime
 14:08:53 up 122 days,  3:47,  2 users,  load average: 0.02, 0.02, 0.05
[root@shccdfrh75vm8 dev]# top
top - 14:09:04 up 122 days,  3:48,  2 users,  load average: 0.02, 0.02, 0.05
```

- `14:08:53`：表示当前时间
- `up 122 days`：表示系统运行时间
- `2 users`：表示登录的用户数。
- `load average: 0.05, 0.03, 0.05`：**系统平均负载**，三个值分别表示最近 1 分钟，5 分钟，15 分钟内的平均负载。

### 什么是平均负载

平均负载就是指单位时间内，系统处于**可运行状态**和**不可中断状态**的平均进程数，也就是**平均活跃进程数**，它和 CPU 使用率并没有直接关系。

- **可运行状态**的进程，是正在使用 CPU 或者正在等待 CPU 的进程，R 状态（Running 或 Runnable）。
- **不可中断状态**的进程，是正处于内核态关键流程中的进程，并且这些流程不可打断。例如等待硬件设备 IO 响应的进程，D 状态（Uninterruptible Sleep，也叫 Disk Sleep）。

**平均负载就可以简单的理解为是平均的活跃进程数**。那么最理想的，就是每个 CPU 都只运行了一个进程。如当平均负载是 2 时，意味着：

- 在有 2 个 CPU 的系统上，意味着所有的 CPU 都刚好被占用。
- 在 4 个 CPU 的系统上，意味着 CPU 有 50% 是空闲的。
- 在 1 个 CPU 的系统上，意味着有一半的进程竞争不到 CPU。

### 平均负载的理想值

**平均负载最理想的情况是等于 CPU 个数**。

查看 CPU 个数：`grep 'model name' /proc/cpuinfo | wc -l`。

**当平均负载比 CPU 个数还大的时候，系统已经出现了过载**。

平均负载有三个值，可以帮助分析**系统负载的趋势**：

- 如果三个值相差不大，说明系统负载很平稳。
- 如果 1 分钟的值远小于 15 分钟的值，说明系统最近 1 分钟的负载在减少，但是过去 15 分钟内负载很大。
- 如果 1 分钟的值远大于 15 分钟的值，说明最近 1 分钟的负载在增加，这种增加可能是临时性的，也有可能持续增加，需要持续观察。一旦 1 分钟的平均负载接近或者超过了 CPU 的个数，就意味着系统正在发生过载的问题。

假设在一个单 CPU 系统上看到平均负载为 1.73，0.60，7.98，那么说明在过去 1 分钟内，系统有 73% 的超载，而在 15 分钟内，有 698% 的超载，从整体趋势来看，系统的负载在降低。

### 平均负载和 CPU 使用率

平均负载不仅包括了**正在使用 CPU**的进程，还包括**等待 CPU** 和**等待 IO** 的进程。

CPU 使用率，是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应。

- CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的；
- IO 密集型进程，等待 IO 也会导致平均负载升高，但 CPU 使用率不一定很高；
- 大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高。

### 平均负载分析

需要安装 `stress` 和 `sysstat` 包，如 `apt install stress sysstat`。

- `stress`：是一个 Linux 系统压力测试工具，可以模拟平均负载升高的场景。
- `sysstat` 包含了常用的 Linux 性能工具，用来监控和分析系统的性能。
  - `mpstat`：是一个常用的多核 CPU 性能分析工具，用来实时**查看每个 CPU** 的性能指标，以及所有 CPU 的平均指标。
  - `pidstat`：是一个常用的进程性能分析工具，用来实时**查看进程**的 CPU、内存、IO 以及上下文切换等性能指标。

### 实战分析

看一下测试前的平均负载情况：

```bash
$ uptime
...,  load average: 0.11, 0.15, 0.09
```

#### CPU 密集型进程

模拟一个 CPU 使用率 100% 的场景：

```bash
$ stress --cpu 1 --timeout 600
```

在第二个终端运行 uptime 查看平均负载的变化情况：

```bash
# -d 参数表示高亮显示变化的区域
$ watch -d uptime
...,  load average: 1.00, 0.75, 0.39
```

在第三个终端运行 mpstat 查看 CPU 使用率的变化情况：

```bash
# -P ALL 表示监控所有 CPU，后面数字 5 表示间隔 5 秒后输出一组数据
$ mpstat -P ALL 5
Linux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU)
13:30:06     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
13:30:11     all   50.05    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.95
13:30:11       0    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00
13:30:11       1  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00
```

从终端二中可以看到，1 分钟的平均负载会慢慢增加到 1.00，而从终端三中还可以看到，正好有一个 CPU 的使用率为 100%，但它的 iowait 只有 0。这说明，平均负载的升高正是由于 CPU 使用率为 100% 。

使用 pidstat 来查看哪个进程导致了 CPU 使用率为 100%：

```bash
# 间隔 5 秒后输出一组数据
# -u 表示显示 CPU 相关的指标
$ pidstat -u 5 1
13:37:07      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
13:37:12        0      2962  100.00    0.00    0.00    0.00  100.00     1  stress
```

#### I/O 密集型进程

模拟 I/O 压力，即不停地执行 sync：

```bash
$ stress -i 1 --timeout 600
```

在第二个终端运行 uptime 查看平均负载的变化情况：

```bash
$ watch -d uptime
...,  load average: 1.06, 0.58, 0.37
```

第三个终端运行 mpstat 查看 CPU 使用率的变化情况：

```bash
$ mpstat -P ALL 5 1
Linux 4.15.0 (ubuntu)     09/22/18     _x86_64_    (2 CPU)
13:41:28     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
13:41:33     all    0.21    0.00   12.07   32.67    0.00    0.21    0.00    0.00    0.00   54.84
13:41:33       0    0.43    0.00   23.87   67.53    0.00    0.43    0.00    0.00    0.00    7.74
13:41:33       1    0.00    0.00    0.81    0.20    0.00    0.00    0.00    0.00    0.00   98.99
```

1 分钟的平均负载会慢慢增加到 1.06，其中一个 CPU 的系统 CPU 使用率升高到了 23.87，而 iowait 高达 67.53%。这说明，平均负载的升高是由于 iowait 的升高。

使用 pidstat 来查看哪个进程导致 iowait 这么高：

```bash
$ pidstat -u 5 1
Linux 4.15.0 (ubuntu)     09/22/18     _x86_64_    (2 CPU)
13:42:08      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
13:42:13        0       104    0.00    3.39    0.00    0.00    3.39     1  kworker/1:1H
13:42:13        0       109    0.00    0.40    0.00    0.00    0.40     0  kworker/0:1H
13:42:13        0      2997    2.00   35.53    0.00    3.99   37.52     1  stress
13:42:13        0      3057    0.00    0.40    0.00    0.00    0.40     0  pidstat
```

#### 大量进程

模拟 8 个进程：

```bash
$ stress -c 8 --timeout 600
```

由于系统只有 2 个 CPU，明显比 8 个进程要少得多，因而，系统的 CPU 处于严重过载状态，平均负载高达 7.97：

```bash
$ uptime
...,  load average: 7.97, 5.93, 3.02
```

再运行 pidstat 来看一下进程的情况：

```bash
$ pidstat -u 5 1
14:23:25      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
14:23:30        0      3190   25.00    0.00    0.00   74.80   25.00     0  stress
14:23:30        0      3191   25.00    0.00    0.00   75.20   25.00     0  stress
14:23:30        0      3192   25.00    0.00    0.00   74.80   25.00     1  stress
14:23:30        0      3193   25.00    0.00    0.00   75.00   25.00     1  stress
14:23:30        0      3194   24.80    0.00    0.00   74.60   24.80     0  stress
14:23:30        0      3195   24.80    0.00    0.00   75.00   24.80     0  stress
14:23:30        0      3196   24.80    0.00    0.00   74.60   24.80     1  stress
14:23:30        0      3197   24.80    0.00    0.00   74.80   24.80     1  stress
14:23:30        0      3200    0.00    0.20    0.00    0.20    0.20     0  pidstat
```

8 个进程在争抢 2 个 CPU，每个进程等待 CPU 的时间（也就是代码块中的 `%wait` 列）高达 75%。这些超出 CPU 计算能力的进程，最终导致 CPU 过载。

### 小结

- 平均负载高有可能是 CPU 密集型进程导致的；
- 平均负载高并不一定代表 CPU 使用率高，还有可能是 I/O 更繁忙了；
- 当发现负载高的时候，你可以使用 mpstat、pidstat 等工具，辅助分析负载的来源。

## CPU 上下文

Linux 是一个多任务操作系统，它支持远大于 CPU 数量的任务同时运行。当然，这些任务实际上并不是真的在同时运行，而是因为系统在很短的时间内，将 CPU 轮流分配给它们，造成多任务同时运行的错觉。

而在每个任务运行前，CPU 都需要知道任务从哪里加载、又从哪里开始运行，也就是说，需要系统事先帮它设置好 **CPU 寄存器和程序计数器**（Program Counter，PC）。

CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。而程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做 **CPU 上下文**。

**CPU 上下文切换**，就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。

### 上下文切换分析

vmstat 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数。

```bash
# 每隔 5 秒输出 1 组数据
# -w 查看每个进程上下文切换的情况
$ vmstat 5
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so     bi    bo   in   cs us sy id  wa st
 0  0      0 7005360  91564 818900    0    0     0     0   25   33  0  0 100  0  0
```

- cs（context switch）：是每秒上下文切换的次数。
- in（interrupt）：则是每秒中断的次数。
- r（Running or Runnable）：是就绪队列的长度，也就是正在运行和等待 CPU 的进程数。
- b（Blocked）：则是处于不可中断睡眠状态的进程数。

要想查看每个进程的详细情况，需要使用 pidstat。给它加上 `-w` 选项，就可以查看每个进程上下文切换的情况了。

```bash
# 每隔 5 秒输出 1 组数据
$ pidstat -w 5
Linux 4.15.0 (ubuntu)  09/23/18  _x86_64_  (2 CPU)
 
08:18:26      UID       PID   cswch/s nvcswch/s  Command
08:18:31        0         1      0.20      0.00  systemd
08:18:31        0         8      5.40      0.00  rcu_sched
```

- cswch，表示每秒自愿上下文切换（voluntary context switches）的次数。**自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换**。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。
- nvcswch，表示每秒非自愿上下文切换（non voluntary context switches）的次数。**非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换**。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。

这个例子中的上下文切换次数 cs 是 33 次，而系统中断次数 in 则是 25 次，而就绪队列长度 r 和不可中断状态进程数 b 都是 0。

#### 实战分析

上下文切换频率是多少次才算正常？

模拟系统多线程调度切换，需要安装 `sysbench` 和 `sysstat`，`sysbench` 一个多线程的基准测试工具，一般用来评估不同系统参数下的数据库负载情况。可以用来模拟系统多线程调度切换的情况。

先用 vmstat 看一下空闲系统的上下文切换次数：

```bash
# 间隔 1 秒后输出 1 组数据
# 第一个 1 表示间隔 1 秒刷新一次
# 第二个 1 表示只输出 1 次结果
$ vmstat 1 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache    si   so    bi    bo   in   cs us sy id wa st
 0  0      0 6984064  92668 830896    0    0     2    19   19   35  1  0 99  0  0
```

上下文切换次数 cs 是 35，而中断次数 in 是 19，r 和 b 都是 0。因为这会儿并没有运行其他任务，所以它们就是空闲系统的上下文切换次数。

在第一个终端里运行 sysbench ，模拟系统多线程调度的瓶颈：

```bash
# 以 10 个线程运行 5 分钟的基准测试，模拟多线程切换的问题
$ sysbench --threads=10 --max-time=300 threads run
```

在第二个终端运行 vmstat ，观察上下文切换情况：

```bash
# 每隔 1 秒输出 1 组数据（需要 Ctrl+C 才结束）
$ vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache     si   so    bi    bo   in   cs    us sy id wa st
 6  0      0 6487428 118240 1292772    0    0     0     0 9019 1398830 16 84  0  0  0
 8  0      0 6487428 118240 1292772    0    0     0     0 10191 1392312 16 84  0  0  0
```

cs 列的上下文切换次数从之前的 35 骤然上升到了 139 万。

其他几个指标：

- r 列：就绪队列的长度已经到了 8，远远超过了系统 CPU 的个数 2，所以肯定会有大量的 CPU 竞争。
- us（user）和 sy（system）列：这两列的 CPU 使用率加起来上升到了 100%，其中系统 CPU 使用率，也就是 sy 列高达 84%，说明 CPU 主要是被内核占用了。
- in 列：中断次数也上升到了 1 万左右，说明中断处理也是个潜在的问题。

综合这几个指标，可以知道，系统的就绪队列过长，也就是正在运行和等待 CPU 的进程数过多，导致了大量的上下文切换，而上下文切换又导致了系统 CPU 的占用率升高。

是什么进程导致了这些问题？

在第三个终端再用 pidstat 来看一下， CPU 和进程上下文切换的情况：

```bash
# 每隔 1 秒输出 1 组数据（需要 Ctrl+C 才结束）
# -w 参数表示输出进程切换指标，而 -u 参数则表示输出 CPU 使用指标
$ pidstat -w -u 1
08:06:33      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
08:06:34        0     10488   30.00  100.00    0.00    0.00  100.00     0  sysbench
08:06:34        0     26326    0.00    1.00    0.00    0.00    1.00     0  kworker/u4:2
 
08:06:33      UID       PID   cswch/s nvcswch/s  Command
08:06:34        0         8     11.00      0.00  rcu_sched
08:06:34        0        16      1.00      0.00  ksoftirqd/1
08:06:34        0       471      1.00      0.00  hv_balloon
08:06:34        0      1230      1.00      0.00  iscsid
08:06:34        0      4089      1.00      0.00  kworker/1:5
08:06:34        0      4333      1.00      0.00  kworker/0:3
08:06:34        0     10499      1.00    224.00  pidstat
08:06:34        0     26326    236.00      0.00  kworker/u4:2
08:06:34     1000     26784    223.00      0.00  sshd
```

CPU 使用率的升高果然是 sysbench 导致的，它的 CPU 使用率已经达到了 100%。但上下文切换则是来自其他进程，包括非自愿上下文切换频率最高的 pidstat ，以及自愿上下文切换频率最高的内核线程 kworker 和 sshd。

pidstat 输出的上下文切换次数，加起来也就几百，比 vmstat 的 139 万明显小了太多。这是怎么回事？

Linux 调度的基本单位实际上是线程，而 sysbench 模拟的也是线程的调度问题，那么，是不是 pidstat 忽略了线程的数据？

运行 man pidstat ，**pidstat 默认显示进程的指标数据，加上 `-t` 参数后，才会输出线程的指标**。

加上 -t 参数，重试一下：

```bash
# 每隔 1 秒输出一组数据（需要 Ctrl+C 才结束）
# -wt 参数表示输出线程的上下文切换指标
$ pidstat -wt 1
08:14:05      UID      TGID       TID   cswch/s nvcswch/s  Command
...
08:14:05        0     10551         -      6.00      0.00  sysbench
08:14:05        0         -     10551      6.00      0.00  |__sysbench
08:14:05        0         -     10552  18911.00 103740.00  |__sysbench
08:14:05        0         -     10553  18915.00 100955.00  |__sysbench
08:14:05        0         -     10554  18827.00 103954.00  |__sysbench
...
```

虽然 sysbench 进程（也就是主线程）的上下文切换次数看起来并不多，但它的子线程的上下文切换次数却有很多。

前面在观察系统指标时，除了上下文切换频率骤然升高，还有一个指标也有很大的变化。是的，正是中断次数。中断次数也上升到了 1 万，但到底是什么类型的中断上升了，现在还不清楚。

pidstat 只是一个进程的性能分析工具，并不提供任何关于中断的详细信息，怎样才能知道中断发生的类型？

**从 `/proc/interrupts` 这个只读文件中读取**。 

```bash
# -d 参数表示高亮显示变化的区域
$ watch -d cat /proc/interrupts
           CPU0       CPU1
...
RES:    2450431    5279697   Rescheduling interrupts
...
```

变化速度最快的是重调度中断（RES），这个中断类型表示，唤醒空闲状态的 CPU 来调度新的任务运行。这是多处理器系统（SMP）中，调度器用来分散任务到不同 CPU 的机制，通常也被称为**处理器间中断**（Inter-Processor Interrupts，IPI）。

所以，这里的中断升高还是因为过多任务的调度问题，跟前面上下文切换次数的分析结果是一致的。

#### 每秒上下文切换多少次才算正常？

这个数值其实取决于系统本身的 CPU 性能。在我看来，如果系统的上下文切换次数比较稳定，那么从数百到一万以内，都应该算是正常的。但当上下文切换次数超过一万次，或者切换次数出现数量级的增长时，就很可能已经出现了性能问题。

这时，你还需要根据上下文切换的类型，再做具体分析。比方说：

- 自愿上下文切换变多了，说明进程都在等待资源，有可能发生了 I/O 等其他问题；
- 非自愿上下文切换变多了，说明进程都在被强制调度，也就是都在争抢 CPU，说明 CPU 的确成了瓶颈；
- 中断次数变多了，说明 CPU 被中断处理程序占用，还需要通过查看 `/proc/interrupts` 文件来分析具体的中断类型。

## CPU 使用率

Linux 将每个 CPU 的时间划分为很短的时间片，再通过调度器轮流分配给各个任务使用，因此造成了多个任务同时运行的错觉。

### CPU 使用率分析

- `top`：显示了系统总体的 CPU 和内存使用情况，以及各个进程的资源使用情况。
- `ps`：只显示了每个进程的资源使用情况。
- `pidstat`：查看进程的 CPU、内存、IO 以及上下文切换等性能指标。

```bash
# 每隔 1 秒输出一组数据，共输出 5 组
$ pidstat 1 5
15:56:02      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
15:56:03        0     15006    0.00    0.99    0.00    0.00    0.99     1  dockerd
 
...
 
Average:      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
Average:        0     15006    0.00    0.99    0.00    0.00    0.99     -  dockerd
```

- 用户态 CPU 使用率（`%usr`）；
- 内核态 CPU 使用率（`%system`）；
- 运行虚拟机 CPU 使用率（`%guest`）；
- 等待 CPU 使用率（`%wait`）；
- 以及总的 CPU 使用率（`%CPU`）。

#### CPU 使用率过高怎么办？

通过 top、ps、pidstat 等工具，能够轻松找到 CPU 使用率较高（比如 100% ）的进程。

那么占用 CPU 的到底是代码里的哪个函数？找到它，才能更高效、更针对性地进行优化。

GDB 在调试程序错误方面很强大。但是并不适合在性能分析的早期应用。因为 GDB 调试程序的过程会中断程序运行，这在线上环境往往是不允许的。所以，GDB 只适合用在性能分析的后期，当你找到了出问题的大致函数后，线下再借助它来进一步调试函数内部的问题。

**`perf` 它以性能事件采样为基础，不仅可以分析系统的各种事件和内核性能，还可以用来分析指定应用程序的性能问题**。

`perf top`，类似于 top，它能够实时显示占用 CPU 时钟最多的函数或者指令，因此可以用来查找热点函数，使用界面如下所示：

```bash
$ perf top
Samples: 833  of event 'cpu-clock', Event count (approx.): 97742399
Overhead  Shared Object       Symbol
   7.28%  perf                [.] 0x00000000001f78a4
   4.72%  [kernel]            [k] vsnprintf
   4.32%  [kernel]            [k] module_get_kallsym
   3.65%  [kernel]            [k] _raw_spin_unlock_irqrestore
```

第一行包含三个数据，分别是采样数（Samples）、事件类型（event）和事件总数量（Event count）。比如这个例子中，perf 总共采集了 833 个 CPU 时钟事件，而总事件数则为 97742399。

再往下看是一个表格式样的数据，每一行包含四列，分别是：

- 第一列 Overhead ，是该符号的性能事件在所有采样中的比例，用百分比来表示。
- 第二列 Shared ，是该函数或指令所在的动态共享对象（Dynamic Shared Object），如内核、进程名、动态链接库名、内核模块名等。
- 第三列 Object ，是动态共享对象的类型。比如 `[.]` 表示用户空间的可执行程序、或者动态链接库，而 `[k]` 则表示内核空间。
- 最后一列 Symbol 是符号名，也就是函数名。当函数名未知时，用十六进制的地址来表示。


使用 perf 需要安装 `linux-tools-common`。

```bash
$ sudo apt install linux-tools-common
```

```bash
# -g 开启调用关系分析，-p 指定进程号 21515
$ perf top -g -p 21515
```

## 不可中断进程和僵尸进程

当 iowait 升高时，进程很可能因为得不到硬件的响应，而长时间处于不可中断的状态。这种进程的状态是 D（Disk Sleep）。


top 和 ps 是最常用的查看进程状态的工具，进程一般有几种状态：

- **R 是 Running 或 Runnable 的缩写**，表示进程在 CPU 的就绪队列中，正在运行或者正在等待运行。
- **D 是 Disk Sleep 的缩写，也就是不可中断状态睡眠（Uninterruptible Sleep）**，一般表示进程正在跟硬件交互，并且交互过程不允许被其他进程或中断打断。
- **Z 是 Zombie 的缩写，表示僵尸进程**，也就是进程实际上已经结束了，但是父进程还没有回收它的资源（比如进程的描述符、PID 等）。
- **S 是 Interruptible Sleep 的缩写，也就是可中断睡眠状态，表示进程因为等待某个事件而被系统挂起**。当进程等待的事件发生时，它会被唤醒并进入 R 状态。
- **I 是 Idle 的缩写，也就是空闲状态**，用在不可中断睡眠的内核线程上。前面说了，硬件交互导致的不可中断进程用 D 表示，但对某些内核线程来说，它们有可能实际上并没有任何负载，用 Idle 正是为了区分这种情况。要注意，D 状态的进程会导致平均负载升高， I 状态的进程却不会。
- **T 或者 t，也就是 Stopped 或 Traced 的缩写，表示进程处于暂停或者跟踪状态**。
- **X，也就是 Dead 的缩写，表示进程已经消亡**，所以你不会在 top 或者 ps 命令中看到它。

不可中断睡眠状态，是为了保证进程数据与硬件状态一致，并且正常情况下，不可中断状态在很短时间内就会结束。但是如果系统或硬件发生了故障，进程可能会在不可中断状态保持很久，导致系统出现大量不可中断进程。这是就需要注意系统是不是出现了 IO 性能问题。

**僵尸进程状态一般持续的时间都很短。父进程回收它之后就会消失，或者父进程退出，由 init 进程回收后消失**。通常当一个进程创建了子进程，它应该调用 wait 或者 waitpid 等待子进程结束，然后回收子进程的资源。子进程在结束时，会向父进程发送 SIGCHLD 信号，所以，父进程可以注册 SIGCJLD 信号的处理函数，异步回收资源。

如果父进程没有回收资源，或子进程执行太快，父进程没来得及处理子进程状态，子进程就已经退出，那么这时的子进程会变成僵尸进程。

### 实战分析

- dstat：可以同时观察系统的 CPU，磁盘 IO，网络以及内存的使用情况。
- strace：跟踪进程系统调用的工具。

**iowait 高并不一定有 IO 性能瓶颈。当系统中有 IO 类型的进程在运行时，iowait 也会很高，但是实际上，磁盘的读写远没有达到性能瓶颈的程度**。

碰到 iowait 升高，先用 dstat，pidstat 确认是不是磁盘 IO 的问题，然后在找出具体是哪个进程导致的。

```bash
# 间隔 1 秒输出 10 组数据
$ dstat 1 10
You did not select any stats, using -cdngy by default.
--total-cpu-usage-- -dsk/total- -net/total- ---paging-- ---system--
usr sys idl wai stl| read  writ| recv  send|  in   out | int   csw
  0   0  96   4   0|1219k  408k|   0     0 |   0     0 |  42   885
  0   0   2  98   0|  34M    0 | 198B  790B|   0     0 |  42   138
  0   0   0 100   0|  34M    0 |  66B  342B|   0     0 |  42   135
  0   0  84  16   0|5633k    0 |  66B  342B|   0     0 |  52   177
  0   3  39  58   0|  22M    0 |  66B  342B|   0     0 |  43   144
  0   0   0 100   0|  34M    0 | 200B  450B|   0     0 |  46   147
  0   0   2  98   0|  34M    0 |  66B  342B|   0     0 |  45   134
  0   0   0 100   0|  34M    0 |  66B  342B|   0     0 |  39   131
  0   0  83  17   0|5633k    0 |  66B  342B|   0     0 |  46   168
  0   3  39  59   0|  22M    0 |  66B  342B|   0     0 |  37   134
```

每当 iowait 升高（wai）时，磁盘的读请求（read）都会很大。这说明 iowait 的升高跟磁盘的读请求有关，很可能就是磁盘读导致的。

如何确定哪个进程在读磁盘呢？

运行 top 命令，观察 D 状态的进程：

```bash
# 观察一会儿按 Ctrl+C 结束
$ top
...
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 4340 root      20   0   44676   4048   3432 R   0.3  0.0   0:00.05 top
 4345 root      20   0   37280  33624    860 D   0.3  0.0   0:00.01 app
 4344 root      20   0   37280  33624    860 D   0.3  0.4   0:00.01 app
...
```

从 top 的输出找到 D 状态进程的 PID，可以发现，这个界面里有两个 D 状态的进程，PID 分别是 4344 和 4345。

`pidstat` 查看这些进程的磁盘读写情况：

```bash
# -d 展示 I/O 统计数据，-p 指定进程号，间隔 1 秒输出 3 组数据
$ pidstat -d -p 4344 1 3
06:38:50      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:38:51        0      4344      0.00      0.00      0.00       0  app
06:38:52        0      4344      0.00      0.00      0.00       0  app
06:38:53        0      4344      0.00      0.00      0.00       0  app
```

`kB_rd` 表示每秒读的 KB 数， `kB_wr` 表示每秒写的 KB 数，`iodelay` 表示 I/O 的延迟（单位是时钟周期）。它们都是 0，那就表示此时没有任何的读写，说明问题不是 4344 进程导致的。同样的方法分析进程 4345，发现它也没有任何磁盘读写。

继续使用 pidstat，但这次去掉进程号，干脆就来观察所有进程的 I/O 使用情况。

```bash
# 间隔 1 秒输出多组数据 (这里是 20 组)
$ pidstat -d 1 20
...
06:48:46      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:47        0      4615      0.00      0.00      0.00       1  kworker/u4:1
06:48:47        0      6080  32768.00      0.00      0.00     170  app
06:48:47        0      6081  32768.00      0.00      0.00     184  app
 
06:48:47      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:48        0      6080      0.00      0.00      0.00     110  app
 
06:48:48      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:49        0      6081      0.00      0.00      0.00     191  app
 
06:48:49      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
 
06:48:50      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:51        0      6082  32768.00      0.00      0.00       0  app
06:48:51        0      6083  32768.00      0.00      0.00       0  app
 
06:48:51      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:52        0      6082  32768.00      0.00      0.00     184  app
06:48:52        0      6083  32768.00      0.00      0.00     175  app
 
06:48:52      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:53        0      6083      0.00      0.00      0.00     105  app
```

观察一会儿可以发现，的确是 app 进程在进行磁盘读，并且每秒读的数据有 32 MB，看来就是 app 的问题。不过，app 进程到底在执行啥 I/O 操作呢？

**strace 正是最常用的跟踪进程系统调用的工具**。从 pidstat 的输出中拿到进程的 PID 号，比如 6082，然后在终端中运行 strace 命令，并用 `-p` 参数指定 PID 号：

```bash
$ strace -p 6082
strace: attach: ptrace(PTRACE_SEIZE, 6082): Operation not permitted
```

先检查一下进程的状态是否正常：

```bash
$ ps aux | grep 6082
root      6082  0.0  0.0      0     0 pts/0    Z+   13:43   0:00 [app] <defunct>
```

果然，进程 6082 已经变成了 Z 状态，也就是僵尸进程。僵尸进程都是已经退出的进程，所以就没法儿继续分析它的系统调用。

**一般不可中断状态的进程是等待 IO 的进程，所以 `ps aux` 找到的 D 状态的进程，一般就是可疑进程。可以使用 strace 分析进程的系统调用，但是如果是僵尸进程，那么不能使用 strace。需要使用 perf 来分析**。

僵尸进程可以使用 pstree 找到父进程后，检查父进程代码，是否在子进程退出之后，会后了资源。

## 软中断

中断其实是一种异步的事件处理机制，可以提高系统的并发处理能力。

中断会打断其他进程的运行，所以中断处理程序要尽可能的快，减少对正常程序运行调度的影响。

并且中断处理程序响应中断时，会临时关闭中断。会导致上一次中断处理完成之前，其他中断无法响应，可能会丢失。

假如你订了 2 份外卖，一份主食和一份饮料，并且是由 2 个不同的配送员来配送。这次你不用时时等待着，两份外卖都约定了电话取外卖的方式。但是，问题又来了。

当第一份外卖送到时，配送员给你打了个长长的电话，商量发票的处理方式。与此同时，第二个配送员也到了，也想给你打电话。

但是很明显，因为电话占线（也就是关闭了中断响应），第二个配送员的电话是打不通的。所以，第二个配送员很可能试几次后就走掉了（也就是丢失了一次中断）。

### 中断丢失的问题

为了解决中断丢失，Linux 将中断处理分为两个阶段：

- **上半部用来快速处理中断**，在中断禁止模式下运行，主要处理跟硬件紧密相关的或时间敏感的工作。
- **下半部用来延迟处理上半部未完成的工作，通常以内核线程的方式运行**。

比如说前面取外卖的例子，上半部就是你接听电话，告诉配送员你已经知道了，其他事儿见面再说，然后电话就可以挂断了；下半部才是取外卖的动作，以及见面后商量发票处理的动作。

这样，第一个配送员不会占用你太多时间，当第二个配送员过来时，照样能正常打通你的电话。

最常见的网卡接收数据包的例子：

1. 网卡收到数据包后，会通过**硬件中断**的方式，通知内核有新数据到了。然后内核就调用中断处理程序来响应。
2. 上半部，快速处理，就是把网卡的数据读到内存中，然后更新硬件寄存器的状态，表示数据读好了，最后发送**软中断**信号，通知下半部处理。
3. 下半部被软中断信号唤醒后，需要从内存找到网络数据，按照网络协议，对数据进行逐层解析和处理，再送给应用程序。

实际上，**上半部会打断 CPU 正在执行的任务，然后立即执行中断处理程序。而下半部以内核线程的方式执行，并且每个 CPU 都对应一个软中断内核线程，名字为 `ksoftirqd/CPU 编号`**，比如说， 0 号 CPU 对应的软中断内核线程的名字就是 `ksoftirqd/0`。

### 查看软中断和内核线程

- `/proc/softirqs` 提供了软中断的运行情况。
- `/proc/interrupts` 提供了硬中断的运行情况。

### 软中断 CPU 使用率升高问题分析

每个 CPU 都有一个软中断内核线程，当软中断事件的频率过高时，内核线程也会因为 CPU 使用率过高而导致软中断处理不及时，引发网络收发延迟，调度缓慢等性能问题。

- sar：是一个系统活动报告工具，可以实时查看系统的当前活动，又可以配置保存和报告历史统计数据
- hping3：是一个可以构造 TCP/IP 协议数据包的工具，可以对系统进行安全审计，防火墙测试等。
- tcpdump：是一个常用网络抓包工具，常用来分许网络问题。

在第一个终端，执行下面的命令运行一个最基本的 Nginx 应用：

```bash
# 运行 Nginx 服务并对外开放 80 端口
$ docker run -itd --name=nginx -p 80:80 nginx
# 确认 Nginx 正常启动
$ curl http://192.168.0.30/
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
```

在第二个终端，我们运行 hping3 命令，来模拟 Nginx 的客户端请求：

```bash
# -S 参数表示设置 TCP 协议的 SYN（同步序列号），-p 表示目的端口为 80
# -i u100 表示每隔 100 微秒发送一个网络帧
# 注：如果你在实践过程中现象不明显，可以尝试把 100 调小，比如调成 10 甚至 1
$ hping3 -S -p 80 -i u100 192.168.0.30
```

到第一个终端，系统响应明显变慢了。因为运行的 hping3 命令，是一个 SYN FLOOD 攻击。

在第一个终端运行 top 命令，看一下系统整体的资源使用情况：

```bash
# top 运行后按数字 1 切换到显示所有 CPU
$ top
top - 10:50:58 up 1 days, 22:10,  1 user,  load average: 0.00, 0.00, 0.00
Tasks: 122 total,   1 running,  71 sleeping,   0 stopped,   0 zombie
# us (user) - 用户空间进程占用 CPU 时间的百分比
# sy (system) - 内核空间进程占用 CPU 时间的百分比
# ni (nice) - 调整过优先级的用户进程占用 CPU 时间的百分比
# id (idle) - CPU 空闲时间的百分比
# wa (I/O wait) - CPU 等待 I/O 操作完成的时间百分比
# hi (hardware irq) - 硬件中断占用的 CPU 时间百分比
# si (software irq) - 软件中断占用的 CPU 时间百分比
# st (steal time) - 虚拟 CPU 被其他操作系统占用的时间百分比
%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni, 96.7 id,  0.0 wa,  0.0 hi,  3.3 si,  0.0 st
%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni, 95.6 id,  0.0 wa,  0.0 hi,  4.4 si,  0.0 st
...
 
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
    7 root      20   0       0      0      0 S   0.3  0.0   0:01.64 ksoftirqd/0
   16 root      20   0       0      0      0 S   0.3  0.0   0:01.97 ksoftirqd/1
 2663 root      20   0  923480  28292  13996 S   0.3  0.3   4:58.66 docker-containe
 3699 root      20   0       0      0      0 I   0.3  0.0   0:00.13 kworker/u4:0
 3708 root      20   0   44572   4176   3512 R   0.3  0.1   0:00.07 top
    1 root      20   0  225384   9136   6724 S   0.0  0.1   0:23.25 systemd
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.03 kthreadd
...
```

- 平均负载全是 0，就绪队列里面只有一个进程（1 running）。
- 每个 CPU 的使用率都挺低，最高的 CPU1 的使用率也只有 4.4%，并不算高。
- 再看进程列表，CPU 使用率最高的进程也只有 0.3%，并不高。

为什么系统的响应变慢了？

两个 CPU 的使用率虽然分别只有 3.3% 和 4.4%，但都用在了软中断上；而从进程列表上也可以看到，CPU 使用率最高的也是软中断进程 ksoftirqd。看起来，软中断有点可疑了。

```bash
$ watch -d cat /proc/softirqs
                    CPU0       CPU1
          HI:          0          0
       TIMER:    1083906    2368646
      NET_TX:         53          9
      NET_RX:    1550643    1916776
       BLOCK:          0          0
    IRQ_POLL:          0          0
     TASKLET:     333637       3930
       SCHED:     963675    2293171
     HRTIMER:          0          0
         RCU:    1542111    1590625
```

通过 `/proc/softirqs` 文件内容的变化情况，可以发现， TIMER（定时中断）、NET_RX（网络接收）、SCHED（内核调度）、RCU（RCU 锁）等这几个软中断都在不停变化。

其中，**NET_RX，也就是网络数据包接收软中断的变化速率最快**。而其他几种类型的软中断，是保证 Linux 调度、时钟和临界区保护这些正常工作所必需的，所以它们有一定的变化倒是正常的。

继续分析网络接收的软中断，第一步应该就是观察系统的网络接收情况。**sar 可以用来查看系统的网络收发情况，还有一个好处是，不仅可以观察网络收发的吞吐量（BPS，每秒收发的字节数），还可以观察网络收发的 PPS，即每秒收发的网络帧数**。

在第一个终端中运行 sar 命令，并添加 `-n DEV` 参数显示网络收发的报告：

```bash
# -n DEV 表示显示网络收发的报告，间隔 1 秒输出一组数据
$ sar -n DEV 1
15:03:46        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
15:03:47         eth0  12607.00   6304.00    664.86    358.11      0.00      0.00      0.00      0.01
15:03:47      docker0   6302.00  12604.00    270.79    664.66      0.00      0.00      0.00      0.00
15:03:47           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
15:03:47    veth9f6bbcd   6302.00  12604.00    356.95    664.66      0.00      0.00      0.00      0.05
```

- 第一列：表示报告的时间。
- 第二列：IFACE 表示网卡。
- 第三、四列：`rxpck/s` 和 `txpck/s` 分别表示每秒接收、发送的网络帧数，也就是 PPS。
- 第五、六列：`rxkB/s` 和 `txkB/s` 分别表示每秒接收、发送的千字节数，也就是 BPS。

对网卡 eth0 来说，每秒接收的网络帧数比较大，达到了 12607，而发送的网络帧数则比较小，只有 6304；每秒接收的千字节数只有 664 KB，而发送的千字节数更小，只有 358 KB。

docker0 和 veth9f6bbcd 的数据跟 eth0 基本一致，只是发送和接收相反，发送的数据较大而接收的数据较小。这是 Linux 内部网桥转发导致的，是系统把 eth0 收到的包转发给 Nginx 服务。

eth0 ：接收的 PPS 比较大，达到 12607，而接收的 BPS 却很小，只有 664 KB。直观来看网络帧应该都是比较小的，稍微计算一下，`664*1024/12607 = 54` 字节，说明平均每个网络帧只有 54 字节，这显然是很小的网络帧，也就是通常所说的**小包问题**。

使用 tcpdump 抓取 eth0 上的包， Nginx 监听在 80 端口，它所提供的 HTTP 服务是基于 TCP 协议的，所以可以指定 TCP 协议和 80 端口精确抓包。

```bash
# -i eth0 只抓取 eth0 网卡，-n 不解析协议名和主机名
# tcp port 80 表示只抓取 tcp 协议并且端口号为 80 的网络帧
$ tcpdump -i eth0 -n tcp port 80
15:11:32.678966 IP 192.168.0.2.18238 > 192.168.0.30.80: Flags [S], seq 458303614, win 512, length 0
...
```

- `192.168.0.2.18238 > 192.168.0.30.80` ，表示网络帧从 192.168.0.2 的 18238 端口发送到 192.168.0.30 的 80 端口，也就是从运行 hping3 机器的 18238 端口发送网络帧，目的为 Nginx 所在机器的 80 端口。
- `Flags [S]` 则表示这是一个 SYN 包。

PPS 超过 12000 的现象，现在可以确认，这就是从 192.168.0.2 这个地址发送过来的 SYN FLOOD 攻击。

从系统的软中断使用率高这个现象出发，通过观察 `/proc/softirqs` 文件的变化情况，判断出软中断类型是网络接收中断；再通过 sar 和 tcpdump ，确认这是一个 SYN FLOOD 问题。

SYN FLOOD 问题最简单的解决方法，就是从交换机或者硬件防火墙中封掉来源 IP，这样 SYN FLOOD 网络帧就不会发送到服务器中。

## 如何分析 CPU 性能瓶颈

### CPU 性能指标

#### CPU 使用率

实际环境中最常见的一个性能指标。

- 用户 CPU 使用率，包括用户态 CPU 使用率（user）和低优先级用户态 CPU 使用率（nice），表示 CPU 在用户态运行的时间百分比。**用户 CPU 使用率高，通常说明有应用程序比较繁忙**。
- 系统 CPU 使用率，表示 CPU 在内核态运行的时间百分比（不包括中断）。系统 CPU 使用率高，说明内核比较繁忙。
- 等待 I/O 的 CPU 使用率，通常也称为 iowait，表示等待 I/O 的时间百分比。iowait 高，通常说明系统与硬件设备的 I/O 交互时间比较长。
- 软中断和硬中断的 CPU 使用率，分别表示内核调用软中断处理程序、硬中断处理程序的时间百分比。它们的使用率高，通常说明系统发生了大量的中断。
- 除了上面这些，还有在虚拟化环境中会用到的窃取 CPU 使用率（steal）和客户 CPU 使用率（guest），分别表示被其他虚拟机占用的 CPU 时间百分比，和运行客户虚拟机的 CPU 时间百分比。

#### 平均负载

平均负载（Load Average），也就是系统的平均活跃进程数。它反应了系统的整体负载情况，主要包括三个数值，分别指过去 1 分钟、过去 5 分钟和过去 15 分钟的平均负载。

**理想情况下，平均负载等于逻辑 CPU 个数，这表示每个 CPU 都恰好被充分利用。如果平均负载大于逻辑 CPU 个数，就表示负载比较重了**。

#### 上下文切换

- 无法获取资源而导致的自愿上下文切换；
- 被系统强制调度导致的非自愿上下文切换。

上下文切换，本身是保证 Linux 正常运行的一项核心功能。但**过多的上下文切换，会将原本运行进程的 CPU 时间，消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，缩短进程真正运行的时间，成为性能瓶颈**。

#### CPU 缓存的命中率

由于 CPU 发展的速度远快于内存的发展，CPU 的处理速度就比内存的访问速度快得多。这样，CPU 在访问内存的时候，免不了要等待内存的响应。为了协调这两者巨大的性能差距，CPU 缓存（通常是多级缓存）就出现了。

CPU 缓存的速度介于 CPU 和内存之间，缓存的是热点的内存数据。根据不断增长的热点数据，这些缓存按照大小不同分为 L1、L2、L3 等三级缓存，其中 L1 和 L2 常用在单核中， L3 则用在多核中。

从 L1 到 L3，三级缓存的大小依次增大，相应的，性能依次降低（当然比内存还是好得多）。而它们的命中率，衡量的是 CPU 缓存的复用情况，命中率越高，则表示性能越好。

### 性能工具

#### 平均负载

1. 先用 uptime，查看系统的平均负载；
2. 用 mpstat 和 pidstat，分别观察了每个 CPU 和每个进程 CPU 的使用情况，进而找出导致平均负载升高的进程。

#### 上下文切换

1. 先用 vmstat ，查看系统的上下文切换次数和中断次数；
2. 然后通过 `pidstat -w`，观察进程的自愿上下文切换和非自愿上下文切换情况；

#### CPU 使用率

1. 先用 top 观察到系统 CPU 使用率，
2. 如果通过 top 和 pidstat，却找不出高 CPU 使用率的进程。从 CPU 使用率不高但处于 Running 状态的进程入手，通过 perf 发现，这个进程的 CPU 使用率很高，是因为它在执行一个短时任务。
3. 再通过 perf 分析，发现这个进程在执行一个短时任务，而这个任务的执行时间占比比较高。

#### 不可中断进程和僵尸进程

1. 先用 top 观察到了 iowait 升高的问题，
2. 如果发现大量的不可中断进程和僵尸进程，用 dstat 查看是否是由磁盘读写导致的。
3. 通过 pidstat 找出相关的进程
4. strace 查看进程系统调用
5. 如果 strace 查看系统调用失败，使用 perf 分析进程调用链

#### 软中断

1. 通过 top 观察系统的软中断 CPU 使用率升高；
2. 接着查看 `/proc/softirqs`，找到几种变化速率较快的软中断；
3. 然后通过 sar 命令，发现是网络小包的问题；
4. 最后再用 tcpdump，找出网络帧的类型和来源。

#### 小结

为了缩小排查范围，通常先运行几个支持指标较多的工具，如 top、vmstat 和 pidstat 

## CPU 性能优化

### 应用程序优化

从应用程序的角度来说，降低 CPU 使用率的最好方法当然是，排除所有不必要的工作，只保留最核心的逻辑。比如减少循环的层次、减少递归、减少动态内存分配等等。

- **编译器优化**：很多编译器都会提供优化选项，适当开启它们，在编译阶段就可以获得编译器的帮助，来提升性能。比如，gcc 就提供了优化选项 -O2，开启后会自动对应用程序的代码进行优化。
- **算法优化**：使用复杂度更低的算法，可以显著加快处理速度。比如，在数据比较大的情况下，可以用 O(nlogn) 的排序算法（如快排、归并排序等），代替 O(n^2) 的排序算法（如冒泡、插入排序等）。
- **异步处理**：使用异步处理，可以避免程序因为等待某个资源而一直阻塞，从而提升程序的并发处理能力。比如，把轮询替换为事件通知，就可以避免轮询耗费 CPU 的问题。
- **多线程代替多进程**：相对于进程的上下文切换，线程的上下文切换并不切换进程地址空间，因此可以降低上下文切换的成本。
- **善用缓存**：经常访问的数据或者计算过程中的步骤，可以放到内存中缓存起来，这样在下次用时就能直接从内存中获取，加快程序的处理速度。

### 系统优化

- **CPU 绑定**：把进程绑定到一个或者多个 CPU 上，可以提高 CPU 缓存的命中率，减少跨 CPU 调度带来的上下文切换问题。
- **CPU 独占**：跟 CPU 绑定类似，进一步将 CPU 分组，并通过 CPU 亲和性机制为其分配进程。这样，这些 CPU 就由指定的进程独占，换句话说，不允许其他进程再来使用这些 CPU。
- **优先级调整**：使用 nice 调整进程的优先级，正值调低优先级，负值调高优先级。优先级的数值含义前面我们提到过，忘了的话及时复习一下。在这里，适当降低非核心应用的优先级，增高核心应用的优先级，可以确保核心应用得到优先处理。
- **为进程设置资源限制**：使用 Linux cgroups 来设置进程的 CPU 使用上限，可以防止由于某个应用自身的问题，而耗尽系统资源。
- **NUMA（Non-Uniform Memory Access）优化**：支持 NUMA 的处理器会被划分为多个 node，每个 node 都有自己的本地内存空间。NUMA 优化，其实就是让 CPU 尽可能只访问本地内存。
- **中断负载均衡**：无论是软中断还是硬中断，它们的中断处理程序都可能会耗费大量的 CPU。开启 irqbalance 服务或者配置 `smp_affinity`，就可以把中断处理过程自动负载均衡到多个 CPU 上。